{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download GloVe from below:\n",
    "#### http://nlp.stanford.edu/data/glove.6B.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('D:/DataScienceCollection/GloVe/Data/tennis_articles_v4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Maria Sharapova has basically no friends as te...\n",
       "1    BASEL, Switzerland (AP), Roger Federer advance...\n",
       "2    Roger Federer has revealed that organisers of ...\n",
       "3    Kei Nishikori will try to end his long losing ...\n",
       "4    Federer, 37, first broke through on tour over ...\n",
       "5    Nadal has not played tennis since he was force...\n",
       "6    Tennis giveth, and tennis taketh away. The end...\n",
       "7    Federer won the Swiss Indoors last week by bea...\n",
       "Name: article_text, dtype: object"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['article_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For now, let's work on Maria Sharapova's Text Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Maria Sharapova has basically no friends as tennis players on the WTA Tour.', \"The Russian player has no problems in openly speaking about it and in a recent interview she said: 'I don't really hide any feelings too much.\", 'I think everyone knows this is my job here.', \"When I'm on the courts or when I'm on the court playing, I'm a competitor and I want to beat every single person whether they're in the locker room or across the net.\", \"So I'm not the one to strike up a conversation about the weather and know that in the next few minutes I have to go and try to win a tennis match.\", \"I'm a pretty competitive girl.\", \"I say my hellos, but I'm not sending any players flowers as well.\", \"Uhm, I'm not really friendly or close to many players.\", \"I have not a lot of friends away from the courts.'\", 'When she said she is not really close to a lot of players, is that something strategic that she is doing?', \"Is it different on the men's tour than the women's tour? '\", 'No, not at all.', \"I think just because you're in the same sport doesn't mean that you have to be friends with everyone just because you're categorized\", \", you're a tennis player,\", \"so you're going to get along with tennis players.\", 'I think every person has different interests.', \"I have friends that have completely different jobs and interests, and I've met them in very different parts of my life.\", \"I think everyone just thinks because we're tennis players we should be the greatest of friends.\", 'But ultimately tennis is just a very small part of what we do.', \"There are so many other things that we're interested in, that we do.'\"]\n"
     ]
    }
   ],
   "source": [
    "row = data['article_text'][0]\n",
    "doc = nlp(row)\n",
    "sentences=[sent.string.strip() for sent in doc.sents]\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataAfterLemmaFilter = []\n",
    "dataAfterPronounFilter = []\n",
    "dataAfterStopwordsFilter =[]\n",
    "dataAfterPunctuationsFilter =[]\n",
    "dataAfterNounFilter =[]\n",
    "cleanSentences =[]\n",
    "\n",
    "for sent in sentences:\n",
    "    docx = nlp(sent)\n",
    "    for token in docx:\n",
    "        dataAfterLemmaFilter.append(token.lemma_)\n",
    "    for token in dataAfterLemmaFilter:\n",
    "        if token != \"-PRON-\":\n",
    "            dataAfterPronounFilter.append(token.lower().strip())    \n",
    "    for token in dataAfterPronounFilter:\n",
    "        if token != stopwords:\n",
    "            dataAfterStopwordsFilter.append(token)\n",
    "    for token in dataAfterStopwordsFilter:\n",
    "        if token not in punctuations:\n",
    "            dataAfterPunctuationsFilter.append(token)\n",
    "    cleanSentences.append(dataAfterPunctuationsFilter)\n",
    "    dataAfterLemmaFilter = []\n",
    "    dataAfterPronounFilter = []\n",
    "    dataAfterStopwordsFilter =[]\n",
    "    dataAfterPunctuationsFilter =[]\n",
    "    dataAfterNounFilter =[]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['maria',\n",
       "  'sharapova',\n",
       "  'have',\n",
       "  'basically',\n",
       "  'no',\n",
       "  'friend',\n",
       "  'as',\n",
       "  'tennis',\n",
       "  'player',\n",
       "  'on',\n",
       "  'the',\n",
       "  'wta',\n",
       "  'tour'],\n",
       " ['the',\n",
       "  'russian',\n",
       "  'player',\n",
       "  'have',\n",
       "  'no',\n",
       "  'problem',\n",
       "  'in',\n",
       "  'openly',\n",
       "  'speak',\n",
       "  'about',\n",
       "  'and',\n",
       "  'in',\n",
       "  'a',\n",
       "  'recent',\n",
       "  'interview',\n",
       "  'say',\n",
       "  'do',\n",
       "  'not',\n",
       "  'really',\n",
       "  'hide',\n",
       "  'any',\n",
       "  'feeling',\n",
       "  'too',\n",
       "  'much'],\n",
       " ['think', 'everyone', 'know', 'this', 'be', 'job', 'here'],\n",
       " ['when',\n",
       "  'be',\n",
       "  'on',\n",
       "  'the',\n",
       "  'court',\n",
       "  'or',\n",
       "  'when',\n",
       "  'be',\n",
       "  'on',\n",
       "  'the',\n",
       "  'court',\n",
       "  'playing',\n",
       "  'be',\n",
       "  'a',\n",
       "  'competitor',\n",
       "  'and',\n",
       "  'want',\n",
       "  'to',\n",
       "  'beat',\n",
       "  'every',\n",
       "  'single',\n",
       "  'person',\n",
       "  'whether',\n",
       "  'be',\n",
       "  'in',\n",
       "  'the',\n",
       "  'locker',\n",
       "  'room',\n",
       "  'or',\n",
       "  'across',\n",
       "  'the',\n",
       "  'net'],\n",
       " ['so',\n",
       "  'be',\n",
       "  'not',\n",
       "  'the',\n",
       "  'one',\n",
       "  'to',\n",
       "  'strike',\n",
       "  'up',\n",
       "  'a',\n",
       "  'conversation',\n",
       "  'about',\n",
       "  'the',\n",
       "  'weather',\n",
       "  'and',\n",
       "  'know',\n",
       "  'that',\n",
       "  'in',\n",
       "  'the',\n",
       "  'next',\n",
       "  'few',\n",
       "  'minute',\n",
       "  'have',\n",
       "  'to',\n",
       "  'go',\n",
       "  'and',\n",
       "  'try',\n",
       "  'to',\n",
       "  'win',\n",
       "  'a',\n",
       "  'tennis',\n",
       "  'match'],\n",
       " ['be', 'a', 'pretty', 'competitive', 'girl'],\n",
       " ['say',\n",
       "  'hello',\n",
       "  'but',\n",
       "  'be',\n",
       "  'not',\n",
       "  'send',\n",
       "  'any',\n",
       "  'player',\n",
       "  'flower',\n",
       "  'as',\n",
       "  'well'],\n",
       " ['uhm',\n",
       "  'be',\n",
       "  'not',\n",
       "  'really',\n",
       "  'friendly',\n",
       "  'or',\n",
       "  'close',\n",
       "  'to',\n",
       "  'many',\n",
       "  'player'],\n",
       " ['have', 'not', 'a', 'lot', 'of', 'friend', 'away', 'from', 'the', 'court'],\n",
       " ['when',\n",
       "  'say',\n",
       "  'be',\n",
       "  'not',\n",
       "  'really',\n",
       "  'close',\n",
       "  'to',\n",
       "  'a',\n",
       "  'lot',\n",
       "  'of',\n",
       "  'player',\n",
       "  'be',\n",
       "  'that',\n",
       "  'something',\n",
       "  'strategic',\n",
       "  'that',\n",
       "  'be',\n",
       "  'do'],\n",
       " ['be',\n",
       "  'different',\n",
       "  'on',\n",
       "  'the',\n",
       "  'man',\n",
       "  \"'s\",\n",
       "  'tour',\n",
       "  'than',\n",
       "  'the',\n",
       "  'woman',\n",
       "  \"'s\",\n",
       "  'tour'],\n",
       " ['no', 'not', 'at', 'all'],\n",
       " ['think',\n",
       "  'just',\n",
       "  'because',\n",
       "  'be',\n",
       "  'in',\n",
       "  'the',\n",
       "  'same',\n",
       "  'sport',\n",
       "  'do',\n",
       "  'not',\n",
       "  'mean',\n",
       "  'that',\n",
       "  'have',\n",
       "  'to',\n",
       "  'be',\n",
       "  'friend',\n",
       "  'with',\n",
       "  'everyone',\n",
       "  'just',\n",
       "  'because',\n",
       "  'be',\n",
       "  'categorize'],\n",
       " ['be', 'a', 'tennis', 'player'],\n",
       " ['so', 'be', 'go', 'to', 'get', 'along', 'with', 'tennis', 'player'],\n",
       " ['think', 'every', 'person', 'have', 'different', 'interest'],\n",
       " ['have',\n",
       "  'friend',\n",
       "  'that',\n",
       "  'have',\n",
       "  'completely',\n",
       "  'different',\n",
       "  'job',\n",
       "  'and',\n",
       "  'interest',\n",
       "  'and',\n",
       "  'have',\n",
       "  'meet',\n",
       "  'in',\n",
       "  'very',\n",
       "  'different',\n",
       "  'part',\n",
       "  'of',\n",
       "  'life'],\n",
       " ['think',\n",
       "  'everyone',\n",
       "  'just',\n",
       "  'think',\n",
       "  'because',\n",
       "  'be',\n",
       "  'tennis',\n",
       "  'player',\n",
       "  'should',\n",
       "  'be',\n",
       "  'the',\n",
       "  'great',\n",
       "  'of',\n",
       "  'friend'],\n",
       " ['but',\n",
       "  'ultimately',\n",
       "  'tennis',\n",
       "  'be',\n",
       "  'just',\n",
       "  'a',\n",
       "  'very',\n",
       "  'small',\n",
       "  'part',\n",
       "  'of',\n",
       "  'what',\n",
       "  'do'],\n",
       " ['there',\n",
       "  'be',\n",
       "  'so',\n",
       "  'many',\n",
       "  'other',\n",
       "  'thing',\n",
       "  'that',\n",
       "  'be',\n",
       "  'interested',\n",
       "  'in',\n",
       "  'that',\n",
       "  'do']]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanSentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introducing GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract word vectors \n",
    "word_embeddings = {} \n",
    "f = open('D:/DataScienceCollection/GloVe/glove.6B.100d.txt', encoding='utf-8') \n",
    "for line in f: \n",
    "    values = line.split() \n",
    "    word = values[0] \n",
    "    coefs = np.asarray(values[1:], dtype='float32')   \n",
    "    word_embeddings[word] = coefs \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting List of Sentences into DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentencesAsDataFrameSeries = pd.DataFrame(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentencesAsDataFrameSeries.columns = ['sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maria Sharapova has basically no friends as te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Russian player has no problems in openly s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I think everyone knows this is my job here.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When I'm on the courts or when I'm on the cour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>So I'm not the one to strike up a conversation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I'm a pretty competitive girl.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I say my hellos, but I'm not sending any playe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Uhm, I'm not really friendly or close to many ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I have not a lot of friends away from the cour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>When she said she is not really close to a lot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Is it different on the men's tour than the wom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>No, not at all.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I think just because you're in the same sport ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>, you're a tennis player,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>so you're going to get along with tennis players.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I think every person has different interests.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>I have friends that have completely different ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I think everyone just thinks because we're ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>But ultimately tennis is just a very small par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>There are so many other things that we're inte...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentences\n",
       "0   Maria Sharapova has basically no friends as te...\n",
       "1   The Russian player has no problems in openly s...\n",
       "2         I think everyone knows this is my job here.\n",
       "3   When I'm on the courts or when I'm on the cour...\n",
       "4   So I'm not the one to strike up a conversation...\n",
       "5                      I'm a pretty competitive girl.\n",
       "6   I say my hellos, but I'm not sending any playe...\n",
       "7   Uhm, I'm not really friendly or close to many ...\n",
       "8   I have not a lot of friends away from the cour...\n",
       "9   When she said she is not really close to a lot...\n",
       "10  Is it different on the men's tour than the wom...\n",
       "11                                    No, not at all.\n",
       "12  I think just because you're in the same sport ...\n",
       "13                          , you're a tennis player,\n",
       "14  so you're going to get along with tennis players.\n",
       "15      I think every person has different interests.\n",
       "16  I have friends that have completely different ...\n",
       "17  I think everyone just thinks because we're ten...\n",
       "18  But ultimately tennis is just a very small par...\n",
       "19  There are so many other things that we're inte..."
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentencesAsDataFrameSeries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning & Basic Computation on each row of Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataAfterLemmaFilter = []\n",
    "dataAfterPronounFilter = []\n",
    "dataAfterStopwordsFilter =[]\n",
    "dataAfterPunctuationsFilter =[]\n",
    "dataAfterNounFilter =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(STOP_WORDS)\n",
    "punctuations = string.punctuation\n",
    "\n",
    "for index,row in sentencesAsDataFrameSeries.iterrows():\n",
    "    docx = nlp(row['sentences'])\n",
    "    for token in docx:\n",
    "        dataAfterLemmaFilter.append(token.lemma_)\n",
    "    for token in dataAfterLemmaFilter:\n",
    "        if token != \"-PRON-\":\n",
    "            dataAfterPronounFilter.append(token.lower().strip())    \n",
    "    for token in dataAfterPronounFilter:\n",
    "        if token != stopwords:\n",
    "            dataAfterStopwordsFilter.append(token)\n",
    "    for token in dataAfterStopwordsFilter:\n",
    "        if token not in punctuations:\n",
    "            dataAfterPunctuationsFilter.append(token)\n",
    "    sentencesAsDataFrameSeries.at[index, \"sentences\"] = dataAfterPunctuationsFilter\n",
    "    dataAfterLemmaFilter = []\n",
    "    dataAfterPronounFilter = []\n",
    "    dataAfterStopwordsFilter =[]\n",
    "    dataAfterPunctuationsFilter =[]\n",
    "    dataAfterNounFilter =[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanSentencesAsListInDataFrame=sentencesAsDataFrameSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[maria, sharapova, have, basically, no, friend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[the, russian, player, have, no, problem, in, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[think, everyone, know, this, be, job, here]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[when, be, on, the, court, or, when, be, on, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[so, be, not, the, one, to, strike, up, a, con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[be, a, pretty, competitive, girl]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[say, hello, but, be, not, send, any, player, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[uhm, be, not, really, friendly, or, close, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[have, not, a, lot, of, friend, away, from, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[when, say, be, not, really, close, to, a, lot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[be, different, on, the, man, 's, tour, than, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[no, not, at, all]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[think, just, because, be, in, the, same, spor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[be, a, tennis, player]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[so, be, go, to, get, along, with, tennis, pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[think, every, person, have, different, interest]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[have, friend, that, have, completely, differe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[think, everyone, just, think, because, be, te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[but, ultimately, tennis, be, just, a, very, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[there, be, so, many, other, thing, that, be, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentences\n",
       "0   [maria, sharapova, have, basically, no, friend...\n",
       "1   [the, russian, player, have, no, problem, in, ...\n",
       "2        [think, everyone, know, this, be, job, here]\n",
       "3   [when, be, on, the, court, or, when, be, on, t...\n",
       "4   [so, be, not, the, one, to, strike, up, a, con...\n",
       "5                  [be, a, pretty, competitive, girl]\n",
       "6   [say, hello, but, be, not, send, any, player, ...\n",
       "7   [uhm, be, not, really, friendly, or, close, to...\n",
       "8   [have, not, a, lot, of, friend, away, from, th...\n",
       "9   [when, say, be, not, really, close, to, a, lot...\n",
       "10  [be, different, on, the, man, 's, tour, than, ...\n",
       "11                                 [no, not, at, all]\n",
       "12  [think, just, because, be, in, the, same, spor...\n",
       "13                            [be, a, tennis, player]\n",
       "14  [so, be, go, to, get, along, with, tennis, pla...\n",
       "15  [think, every, person, have, different, interest]\n",
       "16  [have, friend, that, have, completely, differe...\n",
       "17  [think, everyone, just, think, because, be, te...\n",
       "18  [but, ultimately, tennis, be, just, a, very, s...\n",
       "19  [there, be, so, many, other, thing, that, be, ..."
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanSentencesAsListInDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Matrix Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = []\n",
    "for i in cleanSentences:\n",
    "    if len(i) != 0:\n",
    "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "    else:\n",
    "        v = np.zeros((100,))\n",
    "    sentence_vectors.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mat = np.zeros([len(sentences), len(sentences)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-218-0a718afaece1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m             \u001b[0msim_mat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_vectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence_vectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        if i != j:\n",
    "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract top 10 sentences as the summary\n",
    "for i in range(10):\n",
    "    print(ranked_sentences[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
